{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3) that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A ∧ ¬ B) ∨ (¬ A ∧ B).\n",
    "\n",
    "    Drew on a paper\n",
    "\n",
    "* 2. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "    No probabiliies in output. Add a sigmoid activation function.\n",
    "\n",
    "* 3. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "\n",
    "    Since it's differentiatible.\n",
    "\n",
    "* 4. Name three popular activation functions. Can you draw them?\n",
    "\n",
    "    Logits, tanh, relu\n",
    "\n",
    "* 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "\n",
    "    * What is the shape of the input matrix X? (m, 10)\n",
    "    * What about the shape of the hidden layer’s weight vector W h (10, 50), and the shape of its bias vector b h (10, 1)?\n",
    "    * What is the shape of the output layer’s weight vector W o (50, 3), and its bias vector b o (50, 1)?\n",
    "    * What is the shape of the network’s output matrix Y? (3, 1)\n",
    "    * Write the equation that computes the network’s output matrix Y as a function of X, W h , b h , W o and b o .\n",
    "    \n",
    "    y = (X.W_h + b_h).W_o + b_o\n",
    "    \n",
    "* 6. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "\n",
    "    For spam - 1, logistic\n",
    "    \n",
    "    For mnist - 10, ReLU\n",
    "    \n",
    "    For housing - 1, No activation needed\n",
    "    \n",
    "* 7. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "    BP is calculating from output errors to the last hidden layer, then back to first hidden layer of individual propotions of contribution to the overall error.\n",
    "    \n",
    "    BP is equivalent to reverse-mode autodiff.\n",
    "    \n",
    "* 8. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "    * number of hidden layers\n",
    "    * number of artifical neurons in each layer\n",
    "    * activation functions of each layer\n",
    "    * initializing functions of weights\n",
    "    \n",
    "* 9. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of Chapter 9, try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on).\n",
    "\n",
    "    Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
